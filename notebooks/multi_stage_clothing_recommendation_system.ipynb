{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fcd11be-9d88-4c0c-9d26-6f7d33dbd311",
   "metadata": {},
   "source": [
    "# Multi-Stage clothing recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Retrieval model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7075efc5363fb80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e734d62e5b72a50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcdf132caf91d1c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_transactions = \"C:\\\\Sajat\\\\Egyetem\\\\MSc\\\\Onallo\\\\HM_dataset\\\\transactions_train.csv\"\n",
    "\n",
    "# Number of transactions in original dataset: 31_788_324\n",
    "max_training_transactions_count = 500_000\n",
    "\n",
    "transactions_df = pd.read_csv(\n",
    "    filepath_or_buffer=path_to_transactions,\n",
    "    usecols=[\"customer_id\", \"article_id\", \"t_dat\"],\n",
    "    nrows=max_training_transactions_count,\n",
    ")\n",
    "\n",
    "print(f\"First transaction date: {transactions_df.head(1)['t_dat']}\")\n",
    "print(f\"Last transaction date: {transactions_df.tail(1)['t_dat']}\")\n",
    "\n",
    "transactions_df = transactions_df.drop(\"t_dat\", axis=1)\n",
    "\n",
    "articles_df = transactions_df[[\"article_id\"]].drop_duplicates()\n",
    "customers_df = transactions_df[[\"customer_id\"]].drop_duplicates()\n",
    "\n",
    "number_of_unique_article_ids = len(articles_df)\n",
    "number_of_unique_customer_ids = len(customers_df)\n",
    "\n",
    "print(f\"Number of article IDs: {number_of_unique_article_ids}\")\n",
    "print(f\"Number of customer IDs:  {number_of_unique_customer_ids}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65be76babc880c0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_of_epochs = 1\n",
    "number_of_hidden_linear_layers = 3\n",
    "number_of_neurons_in_layer = 64\n",
    "embedding_vector_dimension = 16\n",
    "half_batch_size = 8"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31207c6a7d4f2b63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7ccfa71362c30d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RetrievalTrainingNegativeBatchSampledPairDataset(Dataset):\n",
    "\n",
    "    def __init__(self, articles_df, customers_df, transactions_df):\n",
    "        self.articles_df = articles_df\n",
    "        self.customers_df = customers_df\n",
    "        self.transactions_df = transactions_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(transactions_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO this maybe can be optimized with moving to a preprocessing step the article search for users, and the negative item index generation could be also done in a smarter and more efficient way\n",
    "        positive_article = self.transactions_df.iloc[idx]\n",
    "\n",
    "        ids_of_articles_purchased_by_user = self.transactions_df[\n",
    "            self.transactions_df[\"customer_id\"] == positive_article[\"customer_id\"]\n",
    "        ][\"article_id\"].drop_duplicates()\n",
    "\n",
    "        indexes_of_negative_articles_of_user = np.where(\n",
    "            ~self.articles_df[\"article_id\"].isin(ids_of_articles_purchased_by_user)\n",
    "        )[0]\n",
    "        \n",
    "        negative_article_index = np.random.choice(indexes_of_negative_articles_of_user)\n",
    "        \n",
    "        positive_article_index = np.where(self.articles_df[\"article_id\"] == positive_article[\"article_id\"])[0][0]\n",
    "        customer_index = np.where(self.customers_df[\"customer_id\"] == positive_article[\"customer_id\"])[0][0]\n",
    "\n",
    "        return (\n",
    "            torch.tensor([\n",
    "                [positive_article_index, customer_index],\n",
    "                [negative_article_index, customer_index],\n",
    "            ]),\n",
    "            torch.tensor([1.0, 0.0]),\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f956a20f3dfafdd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_relu_stack(\n",
    "        unique_input_count,\n",
    "        number_of_hidden_linear_layers,\n",
    "        number_of_neurons_in_layer,\n",
    "        output_vector_dimension\n",
    "):\n",
    "    modules = [\n",
    "        nn.Embedding(\n",
    "            num_embeddings=unique_input_count,\n",
    "            embedding_dim=number_of_neurons_in_layer,\n",
    "        )\n",
    "    ]\n",
    "    for hidden_layer_idx in range(0, number_of_hidden_linear_layers):\n",
    "        if hidden_layer_idx != (number_of_hidden_linear_layers - 1):\n",
    "            modules.append(nn.Linear(number_of_neurons_in_layer, number_of_neurons_in_layer))\n",
    "            modules.append(nn.ReLU())\n",
    "        else:\n",
    "            modules.append(nn.Linear(number_of_neurons_in_layer, output_vector_dimension))\n",
    "    return nn.Sequential(*modules)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "798694d1a3b85930"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            unique_item_input_count,\n",
    "            unique_query_input_count,\n",
    "            number_of_hidden_linear_layers,\n",
    "            number_of_neurons_in_layer,\n",
    "            output_vector_dimension,\n",
    "            similarity_function,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.item_tower_model = create_relu_stack(\n",
    "            unique_input_count=unique_item_input_count + 1,  # add description the reason of this +1 (OOV)\n",
    "            number_of_hidden_linear_layers=number_of_hidden_linear_layers,\n",
    "            number_of_neurons_in_layer=number_of_neurons_in_layer,\n",
    "            output_vector_dimension=output_vector_dimension,\n",
    "        )\n",
    "        self.query_tower_model = create_relu_stack(\n",
    "            unique_input_count=unique_query_input_count + 1,  # add description the reason of this +1 (OOV)\n",
    "            number_of_hidden_linear_layers=number_of_hidden_linear_layers,\n",
    "            number_of_neurons_in_layer=number_of_neurons_in_layer,\n",
    "            output_vector_dimension=output_vector_dimension,\n",
    "        )\n",
    "\n",
    "        self.similarity_function = similarity_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        item_embedding = self.item_tower_model(x[0])\n",
    "        query_embedding = self.query_tower_model(x[1])\n",
    "        similarity = self.similarity_function(item_embedding, query_embedding)\n",
    "        return similarity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f9be7e6ed2c64c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "two_tower_model = TwoTowerModel(\n",
    "    unique_item_input_count=number_of_unique_article_ids,\n",
    "    unique_query_input_count=number_of_unique_customer_ids,\n",
    "    number_of_hidden_linear_layers=number_of_hidden_linear_layers,\n",
    "    number_of_neurons_in_layer=number_of_neurons_in_layer,\n",
    "    output_vector_dimension=embedding_vector_dimension,\n",
    "    similarity_function=nn.CosineSimilarity(),\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "343453d705426366"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cross_entropy_loss_fn = nn.CrossEntropyLoss()\n",
    "two_tower_model_optimizer = torch.optim.Adam(two_tower_model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afcecace479b83bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def training_function(\n",
    "        dataloader,\n",
    "        model,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "):\n",
    "    for epoch in range(number_of_epochs):\n",
    "        size = len(dataloader.dataset)\n",
    "        model.train()\n",
    "        for batch_index, (X, y) in enumerate(dataloader):\n",
    "            try:\n",
    "                x_with_mixed_positives_and_negatives = X.view(-1, 2)\n",
    "                logits = model(x_with_mixed_positives_and_negatives.t())\n",
    "                reshaped_labels = y.view(-1)\n",
    "                loss = loss_fn(logits, reshaped_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if batch_index % 100 == 0:\n",
    "                    loss_value, current = loss.item(), (batch_index + 1) * len(X)\n",
    "                    print(f\"loss: {loss_value:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            except IndexError:\n",
    "                print(f\"Index error for:\")\n",
    "                print(f\"Input: {X}\")\n",
    "                print(f\"Labels: {y}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Other exception: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa161ab3c4f6f25b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_function(\n",
    "    dataloader=DataLoader(\n",
    "        dataset=RetrievalTrainingNegativeBatchSampledPairDataset(\n",
    "            articles_df=articles_df,\n",
    "            customers_df=customers_df,\n",
    "            transactions_df=transactions_df,\n",
    "        ),\n",
    "        batch_size=half_batch_size,\n",
    "    ),\n",
    "    model=two_tower_model,\n",
    "    loss_fn=cross_entropy_loss_fn,\n",
    "    optimizer=two_tower_model_optimizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8420f4cc180df864"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
